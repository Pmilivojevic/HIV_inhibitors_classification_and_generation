{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    models: Path\n",
    "    stats: Path\n",
    "    source_root: Path\n",
    "    processed_root: Path\n",
    "    source_filename: str\n",
    "    processed_filename: List[str]\n",
    "    tuning: bool\n",
    "    params: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hivclass.constants import *\n",
    "from hivclass.utils.main_utils import create_directories, read_yaml\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_file_path = CONFIG_FILE_PATH,\n",
    "        params_file_path = PARAMS_FILE_PATH,\n",
    "        schema_file_path = SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        self.schema = read_yaml(schema_file_path)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params\n",
    "        \n",
    "        create_directories([config.root_dir, config.models, config.stats])\n",
    "        \n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            models=config.models,\n",
    "            stats=config.stats,\n",
    "            source_root=config.source_root,\n",
    "            processed_root=config.processed_root,\n",
    "            source_filename=config.source_filename,\n",
    "            processed_filename=config.processed_filename,\n",
    "            tuning=config.tuning,\n",
    "            params=params\n",
    "        )\n",
    "        \n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-20 00:23:51,684: INFO: __init__: Enabling RDKit 2024.09.5 jupyter extensions]\n",
      "[2025-04-20 00:23:55,857: WARNING: rdkit_descriptors: No normalization for SPS. Feature removed!]\n",
      "[2025-04-20 00:23:55,858: WARNING: rdkit_descriptors: No normalization for AvgIpc. Feature removed!]\n",
      "[2025-04-20 00:23:55,859: WARNING: rdkit_descriptors: No normalization for NumAmideBonds. Feature removed!]\n",
      "[2025-04-20 00:23:55,860: WARNING: rdkit_descriptors: No normalization for NumAtomStereoCenters. Feature removed!]\n",
      "[2025-04-20 00:23:55,860: WARNING: rdkit_descriptors: No normalization for NumBridgeheadAtoms. Feature removed!]\n",
      "[2025-04-20 00:23:55,862: WARNING: rdkit_descriptors: No normalization for NumHeterocycles. Feature removed!]\n",
      "[2025-04-20 00:23:55,862: WARNING: rdkit_descriptors: No normalization for NumSpiroAtoms. Feature removed!]\n",
      "[2025-04-20 00:23:55,863: WARNING: rdkit_descriptors: No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!]\n",
      "[2025-04-20 00:23:55,864: WARNING: rdkit_descriptors: No normalization for Phi. Feature removed!]\n",
      "[2025-04-20 00:23:55,884: WARNING: __init__: Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow']\n",
      "[2025-04-20 00:23:56,148: WARNING: __init__: Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torchdata.datapipes']\n",
      "[2025-04-20 00:23:56,150: WARNING: __init__: Skipped loading modules with transformers dependency. No module named 'transformers']\n",
      "[2025-04-20 00:23:56,153: WARNING: __init__: cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/env/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)]\n",
      "[2025-04-20 00:23:56,155: WARNING: __init__: Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning']\n",
      "[2025-04-20 00:23:56,158: WARNING: __init__: Skipped loading some Jax models, missing a dependency. No module named 'jax']\n",
      "[2025-04-20 00:23:56,162: WARNING: __init__: Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ModelTrainerConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/research/04_model_trainer.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/research/04_model_trainer.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtqdm\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/research/04_model_trainer.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmango\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m Tuner\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/research/04_model_trainer.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mclass\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mModelTrainer\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/research/04_model_trainer.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config: ModelTrainerConfig):\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/research/04_model_trainer.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n",
      "\u001b[1;32m/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/research/04_model_trainer.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/research/04_model_trainer.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mclass\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mModelTrainer\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/research/04_model_trainer.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config: ModelTrainerConfig):\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/research/04_model_trainer.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/HIV_inhibitors_classification_and_generation/research/04_model_trainer.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtrain_val_separation\u001b[39m(\u001b[39mself\u001b[39m, train_dataset):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ModelTrainerConfig' is not defined"
     ]
    }
   ],
   "source": [
    "from hivclass.utils.molecule_dataset import MoleculeDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hivclass.utils.molecule_dataset import MoleculeDataset\n",
    "from hivclass.utils.mol_gnn import MolGNN\n",
    "from hivclass.utils.main_utils import save_json, plot_metric, plot_confusion_matrix, plot_roc_curve\n",
    "import torch \n",
    "from torch_geometric.data import DataLoader\n",
    "from box import ConfigBox\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from mango import Tuner\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def train_val_separation(self, train_dataset):\n",
    "        data_df = pd.read_csv(os.path.join(self.config.processed_root, self.config.processed_filename[3]))\n",
    "        \n",
    "        # data_name_list = os.listdir(os.path.join(self.config.processed_root, self.config.processed_filename[1]))\n",
    "        # data_idxs = [int(name.split('.')[0].split('_')[1]) for name in data_name_list]\n",
    "        # data_labels = [data_df.HIV_active[i] for i in data_idxs]\n",
    "        \n",
    "        train_df, val_df = train_test_split(\n",
    "            data_df,\n",
    "            test_size=self.config.params.data_transformation.val_size,\n",
    "            stratify=data_df.HIV_active,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # train_idxs, val_idxs, _, _ = train_test_split(\n",
    "        #     data_idxs,\n",
    "        #     data_labels,\n",
    "        #     test_size=self.config.model_params.val_size,\n",
    "        #     stratify=data_labels,\n",
    "        #     random_state=42\n",
    "        # )\n",
    "        \n",
    "        train_idxs = train_df.index.tolist()\n",
    "        val_idxs = val_df.index.tolist()\n",
    "        \n",
    "        train = train_dataset.index_select(train_idxs)\n",
    "        val = train_dataset.index_select(val_idxs)\n",
    "        \n",
    "        return train, val\n",
    "    \n",
    "    def train(self, params, epoch, model, train_loader, optimizer, criterion, device):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for i, batch in tqdm(enumerate(train_loader)):\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            preds = model(batch.x.float(), batch.edge_attr.float(), batch.edge_index, batch.batch)\n",
    "            train_preds.extend(np.rint(torch.sigmoid(preds).cpu().detach().numpy()))\n",
    "            train_labels.extend(batch.y.cpu().detach().numpy())\n",
    "            \n",
    "            loss = criterion(torch.squeeze(preds), batch.y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            accuracy = accuracy_score(train_labels, train_preds)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            print()\n",
    "            sys.stdout.write(\n",
    "                \"Epoch:%2d/%2d - Batch:%2d/%2d - train_loss:%.4f - train_accuracy:%.4f\" %(\n",
    "                    epoch,\n",
    "                    params.num_epochs,\n",
    "                    i,\n",
    "                    len(train_loader),\n",
    "                    loss.item(),\n",
    "                    accuracy\n",
    "                )\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        return total_loss / len(train_loader), accuracy\n",
    "    \n",
    "    def validation(self, epoch, model, val_loader, criterion, best_val_loss, stats_path,  device):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader):\n",
    "                batch.to(device)\n",
    "                \n",
    "                preds = model(batch.x.float(), batch.edge_attr.float(), batch.edge_index, batch.batch)\n",
    "                val_preds.extend(torch.round(torch.squeeze(preds)).cpu().detach().numpy())\n",
    "                val_labels.extend(batch.y.cpu().detach().numpy())\n",
    "                \n",
    "                loss = criterion(torch.squeeze(preds), batch.y.float())\n",
    "                total_loss += loss.item()\n",
    "                accuracy = accuracy_score(val_labels, val_preds)\n",
    "            \n",
    "            epoch_loss = total_loss / len(val_loader)\n",
    "            \n",
    "            if epoch_loss < best_val_loss:\n",
    "                report = classification_report(\n",
    "                    val_labels,\n",
    "                    val_preds,\n",
    "                    zero_division=0,\n",
    "                    output_dict=True\n",
    "                )\n",
    "\n",
    "                save_json(\n",
    "                    os.path.join(stats_path, f'report_{epoch}.json'),\n",
    "                    report\n",
    "                )\n",
    "\n",
    "                conf_matrix = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "                plot_confusion_matrix(\n",
    "                    conf_matrix,\n",
    "                    stats_path,\n",
    "                    epoch\n",
    "                )\n",
    "\n",
    "                auc_score = roc_auc_score(val_labels, val_preds)\n",
    "                auc_score_dict = {'auc_score': auc_score}\n",
    "\n",
    "                save_json(\n",
    "                    os.path.join(stats_path, f'auc_score_{epoch}.json'), \n",
    "                    auc_score_dict\n",
    "                )\n",
    "                \n",
    "                plot_roc_curve(\n",
    "                    val_labels,\n",
    "                    val_preds,\n",
    "                    stats_path,\n",
    "                    epoch\n",
    "                )\n",
    "                \n",
    "        return epoch_loss, accuracy\n",
    "    \n",
    "    def train_tuning(self):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"device:\", device)\n",
    "        \n",
    "        dataset = MoleculeDataset(\n",
    "            self.config.source_root,\n",
    "            self.config.processed_root,\n",
    "            self.config.source_filename,\n",
    "            self.config.processed_filename\n",
    "        )\n",
    "        \n",
    "        train_dataset, val_dataset = self.train_val_separation(dataset)\n",
    "        \n",
    "        def train_compose(params):\n",
    "            params = params[0]\n",
    "            \n",
    "            if self.config.tuning:\n",
    "                folder_name = str(len(os.listdir(self.config.stats)) + 1)\n",
    "            else:\n",
    "                folder_name = \"best_params\"\n",
    "            \n",
    "            models_path = os.path.join(self.config.models, folder_name)\n",
    "            stats_path = os.path.join(self.config.stats, folder_name)\n",
    "            \n",
    "            create_directories([models_path, stats_path])\n",
    "            \n",
    "            with open(os.path.join(stats_path, \"params.yaml\"), 'w') as file:\n",
    "                file.write(yaml.dump(params, sort_keys=False))\n",
    "            \n",
    "            params = ConfigBox(params)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "            params[\"model_edge_dim\"] = train_dataset[0].edge_attr.shape[1]\n",
    "            \n",
    "            print(\"Loading model...\")\n",
    "            model_params = ConfigBox({k: v for k, v in params.items() if k.startswith(\"model_\")})\n",
    "            model = MolGNN(feature_size=train_dataset[0].x.shape[1], model_params=model_params)\n",
    "            model = model.to(device)\n",
    "            \n",
    "            weight = torch.tensor([params[\"pos_weight\"]], dtype=torch.float32).to(device)\n",
    "            criterion = torch.nn.BCEWithLogitsLoss(pos_weight=weight)\n",
    "            optimizer = torch.optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=params['learning_rate'],\n",
    "                momentum=params['sgd_momentum'],\n",
    "                weight_decay=params['weight_decay']\n",
    "            )\n",
    "            \n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=params.scheduler_gamma)\n",
    "            \n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            train_accuracies = []\n",
    "            val_accuracies = []\n",
    "            best_val_loss = float('inf')\n",
    "            early_stopping_counter = 0\n",
    "            epochs_range = range(1, params.num_epochs + 1)\n",
    "            \n",
    "            for epoch in tqdm(range(params.num_epochs)):\n",
    "                if early_stopping_counter <= 10:\n",
    "                    train_epoch_loss, train_epoch_acc = self.train(\n",
    "                        params,\n",
    "                        epoch,\n",
    "                        model,\n",
    "                        train_loader,\n",
    "                        optimizer,\n",
    "                        criterion,\n",
    "                        device\n",
    "                    )\n",
    "                    \n",
    "                    train_losses.append(train_epoch_loss)\n",
    "                    train_accuracies.append(train_epoch_acc)\n",
    "                    \n",
    "                    val_epoch_loss, val_epoch_acc = self.validation(\n",
    "                        epoch,\n",
    "                        model,\n",
    "                        val_loader,\n",
    "                        criterion,\n",
    "                        best_val_loss,\n",
    "                        stats_path,\n",
    "                        device\n",
    "                    )\n",
    "                    \n",
    "                    val_losses.append(val_epoch_loss)\n",
    "                    val_accuracies.append(val_epoch_acc)\n",
    "                    \n",
    "                    print(f'Epoch [{epoch+1}/{params.num_epochs}], '\n",
    "                        f'Loss: {train_epoch_loss:.4f}, '\n",
    "                        f'Validation Loss: {val_epoch_loss:.4f}, '\n",
    "                        f'Train Accuracy: {train_epoch_acc:.2f}%, '\n",
    "                        f'Validation Accuracy: {val_epoch_acc:.2f}%')\n",
    "                    \n",
    "                    if float(val_epoch_loss) < best_val_loss:\n",
    "                        torch.save(model.state_dict(), os.path.join(models_path, f'model_{epoch}.pth'))\n",
    "                        best_val_loss = float(val_epoch_loss)\n",
    "                        early_stopping_counter = 0\n",
    "                    else:\n",
    "                        early_stopping_counter += 1\n",
    "                    \n",
    "                    scheduler.step()\n",
    "                else:\n",
    "                    print(\"Early stopping due to no improvement.\")\n",
    "                    \n",
    "                    plot_metric(\n",
    "                        stats_path,\n",
    "                        epochs_range,\n",
    "                        train_losses,\n",
    "                        val_losses,\n",
    "                        'Train Loss',\n",
    "                        'Validation Loss',\n",
    "                    )\n",
    "                    \n",
    "                    \n",
    "                    plot_metric(\n",
    "                        stats_path,\n",
    "                        epochs_range,\n",
    "                        train_accuracies,\n",
    "                        val_accuracies,\n",
    "                        'Train Accuracies',\n",
    "                        'Validation Accuracies',\n",
    "                    )\n",
    "                    \n",
    "                    return [best_val_loss]\n",
    "            \n",
    "            print(f\"Finishing training with best test loss: {best_val_loss}\")\n",
    "\n",
    "            plot_metric(\n",
    "                stats_path,\n",
    "                epochs_range,\n",
    "                train_losses,\n",
    "                val_losses,\n",
    "                'Train Loss',\n",
    "                'Validation Loss',\n",
    "            )\n",
    "            \n",
    "            \n",
    "            plot_metric(\n",
    "                stats_path,\n",
    "                epochs_range,\n",
    "                train_accuracies,\n",
    "                val_accuracies,\n",
    "                'Train Accuracies',\n",
    "                'Validation Accuracies',\n",
    "            )\n",
    "            \n",
    "            return [best_val_loss]\n",
    "        \n",
    "        if self.config.tuning:\n",
    "            print(\"Running hyperparameter search...\")\n",
    "            params = self.config.params.HYPERPARAMETERS\n",
    "            config = dict()\n",
    "            config[\"optimizer\"] = \"Bayesian\"\n",
    "            config[\"num_iteration\"] = params.tuning_iterations[0]\n",
    "            \n",
    "            tuner = Tuner(params, objective=train_compose, conf_dict=config)\n",
    "            \n",
    "            results = tuner.minimize()\n",
    "            \n",
    "            self.config.params['BEST_PARAMETERS'] = results['best_parameters']\n",
    "            best_params = yaml.save_dump(self.config.params, sort_keys=False)\n",
    "            \n",
    "            with open(PARAMS_FILE_PATH, 'w') as file:\n",
    "                file.write(best_params)\n",
    "        else:\n",
    "            params = self.config.params.BEST_PARAMETERS\n",
    "            best_val_loss = train_compose(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    results = model_trainer.train_tuning()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
